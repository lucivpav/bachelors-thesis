% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% slovak thesis in Slovak language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=B,english]{FITthesis}[2012/06/26]

\usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8

\usepackage{graphicx} %graphics files inclusion
\usepackage{amsmath} %advanced maths
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
% \usepackage{amssymb} %additional math symbols

\usepackage{dirtree} %directory tree visualisation

% % list of acronyms
% \usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
% \iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
% \makeglossaries

%\newcommand{\tg}{\mathop{\mathrm{tg}}} %cesky tangens
%\newcommand{\cotg}{\mathop{\mathrm{cotg}}} %cesky cotangens

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% ODTUD DAL VSE ZMENTE
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\department{Katedra teoretické informatiky}
\title{Paralelní implementace dynamického naivního Bayesovského klasifikátoru}
\authorGN{Pavel} %(křestní) jméno (jména) autora
\authorFN{Lučivňák} %příjmení autora
\authorWithDegrees{Pavel Lučivňák} %jméno autora včetně současných akademických titulů
\author{Pavel Lučivňák} %jméno autora bez akademických titulů
\supervisor{Ing. Tomáš Šabata}
%\acknowledgements{Doplňte, máte-li komu a za co děkovat. V~opačném případě úplně odstraňte tento příkaz.}
\abstractCS{V~několika větách shrňte obsah a přínos této práce v~češtině. Po přečtení abstraktu by se čtenář měl mít čtenář dost informací pro rozhodnutí, zda chce Vaši práci číst.}
\abstractEN{Sem doplňte ekvivalent abstraktu Vaší práce v~angličtině.}
\placeForDeclarationOfAuthenticity{V~Praze}
\declarationOfAuthenticityOption{4} %volba Prohlášení (číslo 1-6)
\keywordsCS{Nahraďte seznamem klíčových slov v češtině oddělených čárkou.}
\keywordsEN{Nahraďte seznamem klíčových slov v angličtině oddělených čárkou.}
% \website{http://site.example/thesis} %volitelná URL práce, objeví se v tiráži - úplně odstraňte, nemáte-li URL práce

\begin{document}

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}

\begin{introduction}
	%sem napište úvod Vaší práce
\end{introduction}

\chapter{Cíl práce}

\chapter{Mathematical background}

\section{Probability density function}

Probability density function (PDF) of a continuous random variable determines a likelihood that a given value occurs. Furthermore, \ref{eq:pdf_interval} gives a probability of value being in interval $[a,b]$.

\begin{equation} \label{eq:pdf_interval}
P(a \leq x \leq b) = \int_a^b \text{PDF}(x) dx
\end{equation}

To satisfy the property that $P(-\inf < x < \inf) = 1$, the following has to hold:

\begin{equation}
\int_{-\inf}^{\inf} \text{PDF}(x) dx = 1
\end{equation}

\section{Gaussian distribution}

Gaussian (normal) distribution is defined by a probability density function

\begin{equation} \label{eq:gaussian_pdf}
\text{PDF}_\mathcal{N}(x,\mu,\sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
\end{equation}

The function is defined by two values - mean $\mu$ and variance $\sigma^2$. Figure \ref{fig:gaussian} provides a visualization of the probability density function.

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{gaussian}
 	\caption{Visualization of Gaussian PDF with parameters $\mu=10$ and $\sigma^2=3^2$.}
 	\label{fig:gaussian}
\end{figure}

TODO: mention why normal distribution is so important?

\section{Gaussian mixture distribution}

Combining multiple Gaussian distributions together results in Gaussian mixture distribution. The PDF is given by \ref{eq:gaussian_mixture_pdf}, where $n$ is number of mixtures.

\begin{equation} \label{eq:gaussian_mixture_pdf}
\text{PDF}_{\mathcal{N}^*}(x,\theta) = \sum_{i=1}^n w_i \text{PDF}_{\mathcal{N}}(x,\mu_i,\sigma^2_i)
\end{equation}

Where $w_i$ is a weight of particular normal distribution. The following conditions have to hold:

\begin{equation}
w_i \geq 0, 1 \leq i \leq n
\end{equation}

\begin{equation}
\sum_{i=1}^n w_i = 1
\end{equation}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{gaussian_mixture}
 	\caption{Visualization of a mixture of three Gaussian distributions with parameters $\mu_1=5, \mu_2=10, \mu_3=15$ and $\sigma_{1,2,3}^2=2^2$. Author: Smason79, Wikimedia}
 	\label{fig:gaussian_mixture}
\end{figure}

\chapter{Analysis}

\section{Markov chain}

\subsection{Introduction}
Markov chain describes possible sequences of events in which a probability of being at a state at given time depends only on value of previous state. 

\subsection{Description}

A Markov chain is defined by

\begin{itemize}
\item set of states $S = \{S_1, S_2, \dots, S_N\}$,
\item transition matrix $A \in \textbf R^{N \times N}$,
\item initial probability vector $\pi \in \textbf R^N$.
\end{itemize}

Where $a_{ij}, 1 \leq i,j \leq N$ is a probability of transitioning from state $S_i$ into state $S_j$. $\pi_i$ is a probability that the initial state is $S_i$.

Suppose a sequence of states $q_1, q_2, \dots, q_T$, where $T \geq 2$. At time $t$, where $1 \leq t \leq T-1$, there is a probability $P(q_{t+1}=S_j | q_t=S_i)$. This probability is described by $a_{ij}$.

\subsection{Markov property}
Markov chain satisfies a Markov property. This property says that $P(q_{t+1}=S_i|q_1=s_1,q_2=s_2,\dots,q_t=s_t) = P(q_{t+1}=S_{i}|q_t=s_t)$. In another words, being at state $S_i$ at time $t+1$ only conditionally depends on being at state $s_t$ at time $t$. Further history doesn't influence the probability.

\subsection{Example}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{mc}
 	\caption{Visualization of a Markov chain with three states.}
 	\label{fig:mc}
\end{figure}

As an example, consider a Markov chain (MC) with three states: $S_1 = \text{sunny}$, $S_2 = \text{rainy}$ and $S_3 = \text{cloudy}$. This MC models how weather changes on every day. Suppose the following transition matrix:

\begin{equation}
A =
\begin{bmatrix}
	0.6	& 0.1 & 0.3 \\
	0.1 & 0.7 & 0.2 \\
	0.3 & 0.3 & 0.4
\end{bmatrix}
\end{equation}

If the weather is sunny, tomorrow will be most likely sunny as well. With probability 0.3 it will be cloudy. Rainy weather is very unlikely tomorrow. Consider the following initial probability vector:

\begin{equation}
\pi = \begin{bmatrix} 0.3 \\ 0.2 \\ 0.5 \end{bmatrix}
\end{equation}

The most likely weather on first day of measurement is cloudy.

\subsection{Limitations}

The previous example only takes into account weather at time $t$ to predict weather at time $t+1$. In reality, there are multiple factors that influence what the weather will be like. According to the example, if a weather is sunny now, it is unlikely it will start raining. If we, however, take into account humidity, we can improve the prediction. If we know that humidity is high, even though it was sunny, we can predict it started raining. Hidden Markov Model addresses this issue.

\section{Hidden Markov Model}

\subsection{Introduction}

Hidden Markov Model (HMM) is an extension of Markov chain. The states are now called hidden states and every hidden state has an associated observed state.

\subsection{Description}

HMM is defined by
\begin{itemize}

\item set of hidden states $S = \{S_1, S_2, \dots, S_N\}$,
\item set of observed states $X$,
\item transition matrix $A \in \textbf R^{N \times N}$,
\item observation probability functions $B = \{b_1(o), b_2(o), \dots, b_N(o)\}$,
\item initial probability vector $\pi \in \textbf R^N$.

\end{itemize}

The values of the $X$ can be either discrete or continuous. The values of $S$ are assumed to be discrete. There are numerous different definitions of HMM, such as [TODO], [TODO] or [TODO]. The observation probability function $b_i(o)$ describes a probability $P(x_t = o | q_t = S_i)$. $b_i(o)$ is also called an emission function.


Imagine a walk through a HMM graph. At time point $t$, there is a hidden state $q_t$ and an observed state $x_t$. Value of the observed state conditionally depends on value of the hidden state. The dependency is described by emission function $b_i(o)$.

If $t \neq 1$ and $T > 1$, there is a hidden state $q_i$ that conditionally depends on value of previous hidden state $q_{t-1}$. The dependency is described by transition $a_{ij}$.

If $t = 1$, there is a hidden state $q_1$ that conditionally depends on initial transition $\pi_i$.

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{hmm}
 	\caption{Visualization of a walk through a Hidden Markov Model. At each time point, there is a hidden state $q_t$ and an observed state $x_t$.}
 	\label{fig:hmm}
\end{figure}

\subsection{Operations}

There are two most important operations that can be performed on HMM. Learning and inference.

\subsubsection{Learning}

Learning is used to calculate the model parameters $\{\pi, A, B\} = \lambda$. It is desired to estimate the parameters such that $\prod_{j=1}^{M} P(\textbf x_j | \lambda )$ is maximized. Where $M \in \textbf N$ is a number of sequences to be learned. $\textbf x_j$ is a \emph{j}-th sequence of observed states. In another words, it is desired to maximize the product of probabilities that given sequence of observed states was generated by given model.

\subsubsection{Inference}

Inference returns the most likely sequence of hidden states, given a sequence of observed states.

\subsubsection{Scoring?}

\subsection{Learning}

\subsubsection{Maximum likelihood estimation}

This method uses maximum likelihood estimation (MLE) to compute model parameters. MLE is a technique for finding parameters of a probabilistic model that best describe behavior of a random variable. The method aims to maximize the likelihood of all the learning data.

Formally $\argmax_{\theta \in \Theta} \prod_{j=1}^{M} L(x_j, \theta)$, where:

\begin{itemize}

\item $M$ is number of data points
\item $x_j$ is a j-th data point
\item \emph{L} is a likelihood function (defined further)
\item $\theta$ describes model parameters
\item $\Theta$ is a set of all model parameters

\end{itemize}

Let's examine how one can view parameters of HMM as probabilistic functions of random variables.

\paragraph{Initial transition function}

The initial probability vector $\pi$ can be viewed as a probability function $\pi_i$, assigning a probability to every hidden state $S_i \in S$. This is a function of discrete random variable.

\paragraph{Transition function}

Every row of $A$ defines a probability function $a_{ij}$ with parameter $j$. The function describes a probability of transitioning into state $S_j$ form state $S_i$. $a_{ij}$ is a function of discrete random variable.

\paragraph{Emission function}

The observation probability functions $b_i(o)$ are functions of either discrete or continuous random variable that takes on values in set of observed states $X$. For purposes of this thesis, it is assumed that continuous variables have Gaussian or Gaussian mixture distribution.

\paragraph{Discrete random variables}

Here I present a straightforward way of defining the $L$ likelihood function in case of discrete random variables. $L(x,\theta) = x_{cnt} / M$, where $x_{cnt}$ is the number of times $x$ occurs in learning data. The number of data points is $M$. Since the likelihood function does not depend on parameter $\theta$, there is no expression to optimize.

There is a reason I did not choose any standard discrete probability distribution to define $L$. The random variable does not have to be \textbf Z, nor \textbf N. In fact it can be any abstract object, such as an animal. A type, where comparison between two objects doesn't make sense. Therefore, it wouldn't make sense to assign non zero probability to values that are not specified in learning phase.

As an example, consider the following data: \{dog, bird, dog, cat, bird, dog\}. Figure \ref{fig:discrete_mle_prob} shows a likelihood function $L$ associated with the data.

% TODO: consider two figures side by side
\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{discrete_mle_hist}
 	\caption{Frequency of data in discrete data set.}
 	\label{fig:discrete_mle_hist}
\end{figure}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{discrete_mle_prob}
 	\caption{Likelihood function $L$ of data in discrete data set. Probability is zero at undefined states.}
 	\label{fig:discrete_mle_prob}
\end{figure}

\paragraph{Continuous random variables}
This work considers two kinds of distributions for continuous random variables: Gaussian distribution and Gaussian mixture distribution.

\paragraph{Gaussian distribution}
Normal distribution is defined by $\theta = \{\mu, \sigma^2\}$. The goal is to find parameter $\theta \in \Theta$, such that the product in \ref{eq:gaussian_mle_prod} is maximized.

\begin{equation} \label{eq:gaussian_mle_prod}
\prod_{j=1}^{M} L(x_j, \theta)
\end{equation}

In case of Gaussian distribution, the likelihood function $L$ is defined by

\begin{equation}
L(x, \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
\end{equation}

Taking derivative of \ref{eq:gaussian_mle_prod} with respect to $\mu$ equal to 0 yields

\begin{equation}
\sum_{j=1}^{M}{x_j - M \mu} = 0
\end{equation}

Maximum likelihood estimate of $\mu$ is therefore

\begin{equation}
\mu_{\text{est}} = \overline X_M
\end{equation}

Derivative of \ref{eq:gaussian_mle_prod} with respect to $\sigma^2$ equal to zero results in MLE of $\sigma^2$ to be

\begin{equation}
\sigma^2_{\text{est}} = \frac{1}{M} \sum_{j=1}^{M} {(X_j-\overline X_M)^2}
\end{equation}

Figure \ref{fig:normal_mle} shows an example of MLE on normally distributed random variable.

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{normal_mle}
 	\caption{Histogram of randomly generated data from normal distribution $\mathcal{N}(40,32^2)$. The green curve is a plot of normal distribution with the maximum likelihood estimate of $\theta$ parameters.}
 	\label{fig:normal_mle}
\end{figure}

\paragraph{Gaussian mixture distribution}
TODO: not a MLE!

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{gaussian_mixture_estimate}
 	\caption{Histogram of randomly generated data from two normal distributions $\mathcal{N}(10,5^2)$ and $\mathcal{N}(30,4^2)$. The green curve is the best estimate, found using EM algorithm, of underlying normal mixture distribution. Dotted curves are estimates of particular normal distributions that are part of the Gaussian mixture.}
 	\label{fig:gaussian_mixture_est}
\end{figure}

Given $K$ gaussians, there are $K$ parameters $\theta_1, \theta_2, \dots, \theta_K$ to estimate. Here I provide description of Expectation-Maximization algorithm based on TODO. This algorithm provides an estimate of the aforementioned parameters.

Let's define a membership weight $w_{jk}$, of data point $x_j$ in mixture $k$

\begin{equation}
w_{jk} = \frac{\alpha_k \text{PDF}_{\mathcal{N}}(x_j,\theta_k)}{\sum_{i=1}^K \alpha_i \text{PDF}_{\mathcal{N}}(x_j,\theta_i)}
\end{equation}

Where $\alpha_k$ is a probability that a randomly selected $x_j$ was generated by mixture $k$.

$w_{jk}$ can be thought of as a level of certainty that $x_j$ was generated by mixture $k$. $\sum_{k=1}^K w_{jk} = 1$ holds.

Likelihood of all data points given gaussian mixture parameters is defined as

\begin{equation} \label{eq:gaussian_mixture_likelihood}
l = \sum_{j=1}^{M} \sum_{k=1}^{K} \alpha_k \text{PDF}_{\mathcal{N}}(x_j, \theta_k)
\end{equation}

EM is an iterative algorithm, typically iterating until convergence is detected. Each iteration consists of two parts. An E-step and an M-step.

In the E-step, weight $w_{jk}$ is computed for each data point $x_j$ and mixture $k$.

Define $W_k = \sum_{j=1}^{M} w_{jk}$, the weight of all data points in mixture $k$. In the M-step, new model parameters are calculated as follows

\begin{equation}
\alpha_k^{\text{new}} = \frac{W_k}{M}
\end{equation}

\begin{equation}
\mu_k^{\text{new}} = \frac{\sum_{j=1}^M w_{jk} x_j}{W_k}
\end{equation}

\begin{equation}
{\sigma_k^{2}}^{\text{new}} = \frac{\sum_{j=1}^M w_{jk} (x_j-\mu_k^{\text{new}})^2}{W_k}
\end{equation}

Where $\mu_k^{\text{new}}$ resembles standard equation for estimating mean, and 
${\sigma_k^{2}}^{\text{new}}$ is similar to an equation for estimating variance. The differences are in weighting.

Before the iterations begin, an initial guess of model parameters is necessary. This can be chosen randomly or through a heuristic. One issue with the EM algorithm is that it does not guarantee to find a global optimum.

Termination of the algorithm is determined by checking that the likelihood \ref{eq:gaussian_mixture_likelihood} of all data points hasn't improved enough in between iterations. In another words

\begin{equation}
l_{\text{cur}} - l_{\text{prev}} < \text{tol}
\end{equation}

\subsection{Inference}

Given a sequence of observed states, what is the most likely associated sequence of hidden states? Viterbi algorithm answers this question. Let's define $\alpha_t(i)$ to be the maximum probability of sequence of hidden and observed states up to time point $t$:

\begin{equation} \label{eq:hmm_viterbi_a}
\alpha_t(i) = max_{q_1,q_2,\dots,q_{t-1}} P(q_1,q_2,\dots,q_{t-1},q_t = S_i,x_1,x_2,\dots,x_t | \theta),
\end{equation}

where $\theta$ are parameters of HMM model.

\paragraph{Initialization}

At time point $t=1$, the probability of being at hidden state $S_i$ is simply the initial probability of being at that state and a probability of being at observed state $x_1$ given hidden state $S_i$. In another words:

\begin{equation}
\alpha_1(i) = \pi_i b_i(x_1)
\end{equation}

\paragraph{Recursion}
Taking into account the structure of HMM, the equation \ref{eq:hmm_viterbi_a} can be rewritten recursively as

\begin{equation} \label{eq:hmm_viterbi_a_rec}
\alpha_t(j) = [\max_i \alpha_{t-1}(i)a_{ij}] b_j(x_t)
\end{equation}

Hidden state at particular time point $t$ is determined by

\begin{equation}
s_t = S_{\argmax_i \alpha_{t}(i)}
\end{equation}

\paragraph{Complexity}

At each time point $t$ and hidden state index $j$, the equation \ref{eq:hmm_viterbi_a_rec} loops through every hidden state index $i$. There are $\vert{S}\vert$ hidden states. In addition, the recursion continues for every time point $t$. There are $T$ time points. Thus the overall complexity is
${\vert{S}\vert}^2 T$.

\subsection{Example}
Suppose an extension of an example mentioned in Markov chain chapter. i.e. there are three hidden states of weather: $S_1 = \text{sunny}$, $S_2 = \text{rainy}$ and $S_3 = \text{cloudy}$. Lets introduce a continuous observed random variable describing humidity level. It is easy to imagine that on sunny day, the humidity will be typically lower than on a rainy day. This observable variable can help in predicting future weather.

\subsection{Limitations}
There is only one observed variable, which may not be sufficient for successful prediction in real-life applications. One could increase the dimensionality of the obsereved variable. That would, however, lead to a problem called curse of dimensionality. As we would increase the dimensionality of observed variable, the learning time would increase greatly up to the point where it would no longer be tractable. Dynamic naive Bayesian classifier addresses this issue.

\section{Bayesian networks}

Bayesian networks are a probabilistic graphical model consisting of nodes and edges. The model is a directed acyclic graph (DAG). Every node describes a random variable which can be either discrete or continuous. Edges describe a conditional dependency among the nodes (variables).

As an example, consider a Bayesian network in figure \ref{fig:bn}. Every node represents a discrete random variable with two possible states - $true$ and $false$. It can be imagined that dehydration can cause red skin, but it can also influence presence of cancer. In addition, red skin may signal a lack of water intake, or the skin may be red because of cancer cells. Every node defines a conditional probability table (CPT) given nodes it depends on.

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{bn}
 	\caption{Visualization of a Bayesian network with three states.}
 	\label{fig:bn}
\end{figure}

\begin{table}
\parbox{0.45\linewidth}{
\centering
\begin{tabular}{|c|c|}
\hline
\multicolumn{2}{|c|}{dehydration}  \\
\hline
true & false \\
\hline
0.1 & 0.9 \\
\hline
\end{tabular}
\caption{CPT for $dehydration$}
}
\hfill
\parbox{0.45\linewidth}{
\centering
\begin{tabular}{|c||c|c|}
\hline
input & \multicolumn{2}{c|}{cancer} \\
\hline
dehydration & true & false \\
\hline
true & 0.1 & 0.9 \\
\hline
false & 0.01 & 0.99 \\
\hline
\end{tabular}
\caption{CPT for $cancer$}
}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|c||c|c|}
\hline
\multicolumn{2}{|c||}{input} & \multicolumn{2}{c|}{red skin} \\
\hline
cancer & dehydration & true & false \\
\hline
true & true & 0.4 & 0.6 \\
\hline
true & false & 0.3 & 0.7 \\
\hline
false & true & 0.4 & 0.6 \\
\hline
false & false & 0.1 & 0.9 \\
\hline
\end{tabular}
\caption{CPT for $red$ $skin$}
\end{table}

\section{Naive Bayesian classifier}

Naive Bayesian classifier is a classifier (model that assigns class labels to given inputs) where features (random variables) are assumed to be conditionally independent from each other.

Imagine we would like to classify a class $C_k$ given feature vector $\textbf x$:

\begin{equation}
P(C_k | x_1, x_2, \dots, x_n).
\end{equation}

Using Bayes' theorem, the conditional probability can be rewritten as

\begin{equation}
P(C_k | \textbf x) = \frac{P(C_k,\textbf x)}{P(\textbf x)} = \frac{P(C_k) P(\textbf x | C_k)}{P(\textbf x)}.
\end{equation}

Because the features are assumed to be conditionally independent from each other, the probability in numerator can be rewritten as

\begin{equation}
P(\textbf x | C_k) = P(x_1|C_k) P(x_2|C_k) \cdots P(x_n|C_k).
\end{equation}

Because of the independence assumption, learning of the classifier can be performed in short time compared to the learning time required by other classifier types ref:TODO.

\section{Dynamic naive Bayesian classifier}

\subsection{Description}

Dynamic naive Bayesian classifier (DNBC) is an extension of Hidden Markov Model (HMM). The difference is that there may be a number of observed variables. In contrast, HMM is defined for only one observed variable. Figure \ref{fig:dnbc} demonstrates model structure with 2 observed variables. DNBC is defined by

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{dnbc}
 	\caption{Visualization of a Dynamic naive Bayesian classifier with two observed variables. At each time point, there is a hidden state $s_i$ and two observed states: $x_i^1$ and $x_i^2$.}
 	\label{fig:dnbc}
\end{figure}

\begin{enumerate}

\item{Number of observed variables $N$}
\item{Initial transition function $I: S \mapsto \textbf R$}
\item{Transition function $T: S \times S \mapsto \textbf R$}
\item{Emission functions $E^j: S \times X^j \mapsto \textbf R, j \in [1,N]$}
\item{Set of hidden states $S$}
\item{Sets of observed states $X^j, j \in [1,N]$}

\end{enumerate}

Each set of observed variables contains a set of observed states. This set can be either discrete or continuous. The set of hidden states is assumed to be discrete.

Given the following:

\begin{enumerate}

\item A number $L \in [1, \inf)$
\item A sequence of hidden states of length \emph{L}
\item Sequences of observed states of length \emph{L}
\item An index \emph{i} $\in [1, L]$: one can think of this as a discrete point in time

\end{enumerate}

At time point \emph{i}, there is a hidden state $s_i$ and $N$ observed states $x^j_i$ where $j \in [1,N]$. Value of every observed state depends on value of the hidden state. The dependency is described by emission function $E^j$. An important property of DNBC is that the observed variables are assumed to be independent. Therefore, DNBC does not define any dependency function between two observed variables.

If $i \neq 1$ and $L > 1$, there is a hidden state $s_i$ that depends on value of previous hidden state $s_{i-1}$. The dependency is described by transition function \emph{T}.

If $i = 1$, there is a hidden state $s_1$ that depends on initial transition function \emph{I}.

\subsection{Operations}

There are two most important operations that can be performed on DNBC. Learning and inference.

\subsubsection{Learning}

Learning is used to calculate the model parameters $\{I,T,E^1,E^2,\dots,E^N\} = \lambda$. It is desired to estimate the parameters such that $\prod_{j=1}^{M} P(\textbf x_j | \lambda )$ is maximized. Where $M \in \textbf N$ is a number of sequences to be learned. $\textbf x_j$ is a \emph{j}-th sequence of observed states. In another words, it is desired to maximize the product of probabilities that given sequence of observed states was generated by given model.

\subsubsection{Inference}

Inference returns the most likely sequence of hidden states, given sequences of observed states. Each sequence of observed states corresponds to particular observed variable. Each sequence thus has the same number of elements.

\subsection{Learning}

\subsubsection{Maximum likelihood estimation}

This learning approach is similar to one described in case of HMM. The only difference is that there are multiple emission functions $E^j$ where $j \in [1,N]$. Every emission function is defined as $E^j: S \times X^j \mapsto \textbf R$. For every $j$ and hidden state $s \in S$, there is a probability function $X^j \mapsto \textbf R$. This function can be represented by continuous probabilistic model. Therefore, there is now $N \times \vert{S}\vert$ continuous distributions to be learned. Each distribution is assumed to be Gaussian or Gaussian mixture.

\subsection{Inference}

Inference algorithm is again Viterbi. It is simply extended to support multiple observed variables. Let $x_t^j$ be a sequence of observed states of variable $j$ up to time point $t$.

\begin{equation}
x_t^j = x_1^j,x_2^j,\dots,x_t^j
\end{equation}

Let's define $a_t(i)$ to be the maximum probability of sequence of hidden and sets of observed states up to time point $t$:

\begin{equation} \label{eq:dnbc_viterbi_a}
a_t(i) = max_{s_1,s_2,\dots,s_{t-1}} P(s_1,s_2,\dots,s_{t-1},s_t = S_i,x_t^1,x_t^2,\dots,x_t^N| \theta)
\end{equation}

Where $S_i$ is a function $\textbf N \mapsto S$, assigning a unique index to each hidden state in $S$. $\theta$ are parameters of DNBC model.

\paragraph{Initialization}

At time point $t=1$, the probability of being at hidden state $S_i$ is equal to the initial probability of being at that state and probabilities of being at observed state $x_1^j$ given hidden state $S_i$. In another words:

\begin{equation}
a_1(i) = I(S_i) \prod_{j=1}^N E^j(S_i,x^j_1)
\end{equation}

\paragraph{Recursion}
Taking into account the structure of DNBC, the equation \ref{eq:dnbc_viterbi_a} can be rewritten recursively as

\begin{equation} \label{eq:dnbc_viterbi_a_rec}
a_t(j) = [\max_i a_{t-1}(i)T(S_i,S_j)] \prod_{k=1}^N E^k(S_j,x^k_t)
\end{equation}

Hidden state at particular time point $t$ is determined by

\begin{equation}
s_t = S_{\argmax_i a_{t}(i)}
\end{equation}

\paragraph{Complexity}

At each time point $t$ and hidden state index $j$, the equation \ref{eq:dnbc_viterbi_a_rec} loops through every hidden state index $i$ and every observed variable index $k$. There are $\vert{S}\vert$ hidden states and $N$ observed variables. In addition, the recursion continues for every time point $t$. There are $L$ time points. Thus the overall time complexity is
${\vert{S}\vert}(\vert{S}\vert+N) L$.

\chapter{Návrh}

\chapter{Realizace}

\begin{conclusion}
	%sem napište závěr Vaší práce
\end{conclusion}

\bibliographystyle{csn690}
\bibliography{mybibliographyfile}

\appendix

\chapter{Seznam použitých zkratek}
% \printglossaries
\begin{description}
	\item[GUI] Graphical user interface
	\item[XML] Extensible markup language
\end{description}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % Tuto kapitolu z výsledné práce ODSTRAŇTE.
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% 
% \chapter{Návod k~použití této šablony}
% 
% Tento dokument slouží jako základ pro napsání závěrečné práce na Fakultě informačních technologií ČVUT v~Praze.
% 
% \section{Výběr základu}
% 
% Vyberte si šablonu podle druhu práce (bakalářská, diplomová), jazyka (čeština, angličtina) a kódování (ASCII, \mbox{UTF-8}, \mbox{ISO-8859-2} neboli latin2 a nebo \mbox{Windows-1250}). 
% 
% V~české variantě naleznete šablony v~souborech pojmenovaných ve formátu práce\_kódování.tex. Typ může být:
% \begin{description}
% 	\item[BP] bakalářská práce,
% 	\item[DP] diplomová (magisterská) práce.
% \end{description}
% Kódování, ve kterém chcete psát, může být:
% \begin{description}
% 	\item[UTF-8] kódování Unicode,
% 	\item[ISO-8859-2] latin2,
% 	\item[Windows-1250] znaková sada 1250 Windows.
% \end{description}
% V~případě nejistoty ohledně kódování doporučujeme následující postup:
% \begin{enumerate}
% 	\item Otevřete šablony pro kódování UTF-8 v~editoru prostého textu, který chcete pro psaní práce použít -- pokud můžete texty s~diakritikou normálně přečíst, použijte tuto šablonu.
% 	\item V~opačném případě postupujte dále podle toho, jaký operační systém používáte:
% 	\begin{itemize}
% 		\item v~případě Windows použijte šablonu pro kódování \mbox{Windows-1250},
% 		\item jinak zkuste použít šablonu pro kódování \mbox{ISO-8859-2}.
% 	\end{itemize}
% \end{enumerate}
% 
% 
% V~anglické variantě jsou šablony pojmenované podle typu práce, možnosti jsou:
% \begin{description}
% 	\item[bachelors] bakalářská práce,
% 	\item[masters] diplomová (magisterská) práce.
% \end{description}
% 
% \section{Použití šablony}
% 
% Šablona je určena pro zpracování systémem \LaTeXe{}. Text je možné psát v~textovém editoru jako prostý text, lze však také využít specializovaný editor pro \LaTeX{}, např. Kile.
% 
% Pro získání tisknutelného výstupu z~takto vytvořeného souboru použijte příkaz \verb|pdflatex|, kterému předáte cestu k~souboru jako parametr. Vhodný editor pro \LaTeX{} toto udělá za Vás. \verb|pdfcslatex| ani \verb|cslatex| \emph{nebudou} s~těmito šablonami fungovat.
% 
% Více informací o~použití systému \LaTeX{} najdete např. v~\cite{wikilatex}.
% 
% \subsection{Typografie}
% 
% Při psaní dodržujte typografické konvence zvoleného jazyka. České \uv{uvozovky} zapisujte použitím příkazu \verb|\uv|, kterému v~parametru předáte text, jenž má být v~uvozovkách. Anglické otevírací uvozovky se v~\LaTeX{}u zadávají jako dva zpětné apostrofy, uzavírací uvozovky jako dva apostrofy. Často chybně uváděný symbol "{} (palce) nemá s~uvozovkami nic společného.
% 
% Dále je třeba zabránit zalomení řádky mezi některými slovy, v~češtině např. za jednopísmennými předložkami a spojkami (vyjma \uv{a}). To docílíte vložením pružné nezalomitelné mezery -- znakem \texttt{\textasciitilde}. V~tomto případě to není třeba dělat ručně, lze použít program \verb|vlna|.
% 
% Více o~typografii viz \cite{kobltypo}.
% 
% \subsection{Obrázky}
% 
% Pro umožnění vkládání obrázků je vhodné použít balíček \verb|graphicx|, samotné vložení se provede příkazem \verb|\includegraphics|. Takto je možné vkládat obrázky ve formátu PDF, PNG a JPEG jestliže používáte pdf\LaTeX{} nebo ve formátu EPS jestliže používáte \LaTeX{}. Doporučujeme preferovat vektorové obrázky před rastrovými (vyjma fotografií).
% 
% \subsubsection{Získání vhodného formátu}
% 
% Pro získání vektorových formátů PDF nebo EPS z~jiných lze použít některý z~vektorových grafických editorů. Pro převod rastrového obrázku na vektorový lze použít rasterizaci, kterou mnohé editory zvládají (např. Inkscape). Pro konverze lze použít též nástroje pro dávkové zpracování běžně dodávané s~\LaTeX{}em, např. \verb|epstopdf|.
% 
% \subsubsection{Plovoucí prostředí}
% 
% Příkazem \verb|\includegraphics| lze obrázky vkládat přímo, doporučujeme však použít plovoucí prostředí, konkrétně \verb|figure|. Například obrázek \ref{fig:float} byl vložen tímto způsobem. Vůbec přitom nevadí, když je obrázek umístěn jinde, než bylo původně zamýšleno -- je tomu tak hlavně kvůli dodržení typografických konvencí. Namísto vynucování konkrétní pozice obrázku doporučujeme používat odkazování z~textu (dvojice příkazů \verb|\label| a \verb|\ref|).
% 
% \begin{figure}\centering
% 	\includegraphics[width=0.5\textwidth, angle=30]{cvut-logo-bw}
% 	\caption[Příklad obrázku]{Ukázkový obrázek v~plovoucím prostředí}\label{fig:float}
% \end{figure}
% 
% \subsubsection{Verze obrázků}
% 
% % Gnuplot BW i barevně
% Může se hodit mít více verzí stejného obrázku, např. pro barevný či černobílý tisk a nebo pro prezentaci. S~pomocí některých nástrojů na generování grafiky je to snadné.
% 
% Máte-li například graf vytvořený v programu Gnuplot, můžete jeho černobílou variantu (viz obr. \ref{fig:gnuplot-bw}) vytvořit parametrem \verb|monochrome dashed| příkazu \verb|set term|. Barevnou variantu (viz obr. \ref{fig:gnuplot-col}) vhodnou na prezentace lze vytvořit parametrem \verb|colour solid|.
% 
% \begin{figure}\centering
% 	\includegraphics{gnuplot-bw}
% 	\caption{Černobílá varianta obrázku generovaného programem Gnuplot}\label{fig:gnuplot-bw}
% \end{figure}
% 
% \begin{figure}\centering
% 	\includegraphics{gnuplot-col}
% 	\caption{Barevná varianta obrázku generovaného programem Gnuplot}\label{fig:gnuplot-col}
% \end{figure}
% 
% 
% \subsection{Tabulky}
% 
% Tabulky lze zadávat různě, např. v~prostředí \verb|tabular|, avšak pro jejich vkládání platí to samé, co pro obrázky -- použijte plovoucí prostředí, v~tomto případě \verb|table|. Například tabulka \ref{tab:matematika} byla vložena tímto způsobem.
% 
% \begin{table}\centering
% 	\caption[Příklad tabulky]{Zadávání matematiky}\label{tab:matematika}
% 	\begin{tabular}{|l|l|c|c|}\hline
% 		Typ		& Prostředí		& \LaTeX{}ovská zkratka	& \TeX{}ovská zkratka	\tabularnewline \hline \hline
% 		Text		& \verb|math|		& \verb|\(...\)|	& \verb|$...$|		\tabularnewline \hline
% 		Displayed	& \verb|displaymath|	& \verb|\[...\]|	& \verb|$$...$$|	\tabularnewline \hline
% 	\end{tabular}
% \end{table}
% 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\chapter{Obsah přiloženého CD}

%upravte podle skutecnosti

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{stručný popis obsahu CD}.
		.1 exe\DTcomment{adresář se spustitelnou formou implementace}.
		.1 src.
		.2 impl\DTcomment{zdrojové kódy implementace}.
		.2 thesis\DTcomment{zdrojová forma práce ve formátu \LaTeX{}}.
		.1 text\DTcomment{text práce}.
		.2 thesis.pdf\DTcomment{text práce ve formátu PDF}.
		.2 thesis.ps\DTcomment{text práce ve formátu PS}.
	}
\end{figure}

\end{document}
