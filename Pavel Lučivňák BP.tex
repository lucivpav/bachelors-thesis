% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% slovak thesis in Slovak language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=B,english]{FITthesis}[2012/06/26]

\usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8

\usepackage{graphicx} %graphics files inclusion
\usepackage{amsmath} %advanced maths
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
% \usepackage{amssymb} %additional math symbols

\usepackage{dirtree} %directory tree visualisation
\usepackage{listings} %source code visualisation
\lstset{breaklines}

% % list of acronyms
% \usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
% \iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
% \makeglossaries

%\newcommand{\tg}{\mathop{\mathrm{tg}}} %cesky tangens
%\newcommand{\cotg}{\mathop{\mathrm{cotg}}} %cesky cotangens

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% ODTUD DAL VSE ZMENTE
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\department{Katedra teoretické informatiky}
\title{Paralelní implementace dynamického naivního Bayesovského klasifikátoru}
\authorGN{Pavel} %(křestní) jméno (jména) autora
\authorFN{Lučivňák} %příjmení autora
\authorWithDegrees{Pavel Lučivňák} %jméno autora včetně současných akademických titulů
\author{Pavel Lučivňák} %jméno autora bez akademických titulů
\supervisor{Ing. Tomáš Šabata}
%\acknowledgements{Doplňte, máte-li komu a za co děkovat. V~opačném případě úplně odstraňte tento příkaz.}
\abstractCS{V~několika větách shrňte obsah a přínos této práce v~češtině. Po přečtení abstraktu by se čtenář měl mít čtenář dost informací pro rozhodnutí, zda chce Vaši práci číst.}
\abstractEN{Sem doplňte ekvivalent abstraktu Vaší práce v~angličtině.}
\placeForDeclarationOfAuthenticity{V~Praze}
\declarationOfAuthenticityOption{4} %volba Prohlášení (číslo 1-6)
\keywordsCS{Nahraďte seznamem klíčových slov v češtině oddělených čárkou.}
\keywordsEN{Nahraďte seznamem klíčových slov v angličtině oddělených čárkou.}
% \website{http://site.example/thesis} %volitelná URL práce, objeví se v tiráži - úplně odstraňte, nemáte-li URL práce

\begin{document}

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}

\begin{introduction}
	%sem napište úvod Vaší práce
\end{introduction}

\chapter{Cíl práce}

\chapter{Mathematical background}

\section{Probability density function}

Probability density function (PDF) of a continuous random variable determines a likelihood that a given value occurs. Furthermore, \ref{eq:pdf_interval} gives a probability of value being in interval $[a,b]$.

\begin{equation} \label{eq:pdf_interval}
P(a \leq x \leq b) = \int_a^b \text{PDF}(x) dx
\end{equation}

To satisfy the property that $P(-\inf < x < \inf) = 1$, the following has to hold:

\begin{equation}
\int_{-\inf}^{\inf} \text{PDF}(x) dx = 1.
\end{equation}

\section{Gaussian distribution}

Gaussian (normal) distribution is defined by a probability density function

\begin{equation} \label{eq:gaussian_pdf}
\text{PDF}_{\mathcal{N}(\mu,\sigma^2)}(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2 \sigma^2}}.
\end{equation}

The distribution is defined by two parameters - mean $\mu$ and variance $\sigma^2$. Figure \ref{fig:gaussian} provides a visualization of the probability density function.

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{gaussian}
 	\caption{Visualization of Gaussian PDF with parameters $\mu=10$ and $\sigma^2=3^2$.}
 	\label{fig:gaussian}
\end{figure}

Normal distribution is one of the most widespread distribution found in nature \cite{gaussian-distribution-widespread}. Imagine $n$ independent identically distributed (i.i.d.) random variables. For large $n > 30$, both sum and average of the variables converge to normal distribution. This phenomenon occurs because of the Central Limit Theorem. See book \textit{Introduction to Probability} \cite{clt-proof} for a proof.

\section{Gaussian mixture distribution}

Combining multiple Gaussian distributions together results in Gaussian mixture distribution. The PDF is given by \ref{eq:gaussian_mixture_pdf}, where $n$ is number of components.

\begin{equation} \label{eq:gaussian_mixture_pdf}
\text{PDF}_{\mathcal{N}^*(\theta)}(x) = \sum_{i=1}^n w_i \text{PDF}_{\mathcal{N}(\mu_i,\sigma^2_i)}(x),
\end{equation}

where $w_i$ is a weight of particular component. The following conditions have to hold:

\begin{equation}
w_i \geq 0, 1 \leq i \leq n,
\end{equation}

\begin{equation}
\sum_{i=1}^n w_i = 1.
\end{equation}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{gaussian_mixture}
 	\caption{Visualization of a mixture with three components with parameters $\mu_1=5, \mu_2=10, \mu_3=15$ and $\sigma_{1,2,3}^2=2^2$. Weights are $w_{1,2,3} = \frac{1}{3}$. Author: Smason79 \cite{gaussian-mixture}.}
 	\label{fig:gaussian_mixture}
\end{figure}

\chapter{Parameter estimation}
\label{parameter-estimation-chapter}
In context of this chapter, the goal of parameter estimation is to estimate parameters of a probability distribution in a way that describes given data the most.

\section{Maximum likelihood estimation}
Maximum likelihood estimation (MLE) is a technique for finding parameters of a probabilistic model that best describe behavior of a random variable. The method aims to maximize the likelihood of all the learning data.

Formally $\argmax_{\theta \in \Theta} \prod_{j=1}^{M} L(x_j, \theta)$, where:

\begin{itemize}

\item $M$ is number of data points,
\item $X = \{x_1, x_2, \dots, x_M\}$ is a learning set of data points,
\item \emph{L} is a likelihood function (defined further),
\item $\theta$ describes model parameters,
\item $\Theta$ is a set of all model parameters.

\end{itemize}

\section{Discrete distribution}

Here I present a straightforward way of defining the $L$ likelihood function in case of discrete random variables. $L(x,\theta) = x_{cnt} / M$, where $x_{cnt}$ is the number of times $x \in X$ occurs in learning data. The number of data points is $M$. Since the likelihood function does not depend on parameter $\theta$, there is no expression to optimize.

There is a reason I did not choose any standard discrete probability distribution to define $L$. The random variable does not have to be \textbf Z, nor \textbf N. In fact it can be any abstract object, such as an animal. A type, where comparison between two objects doesn't make sense. Therefore, it wouldn't make sense to assign non zero probability to values that are not specified in learning phase.

As an example, consider the following data: \{dog, bird, dog, cat, bird, dog\}. Figure \ref{fig:discrete_mle_prob} shows a likelihood function $L$ associated with the data.

% TODO: consider two figures side by side
\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{discrete_mle_hist}
 	\caption{Frequency of data in discrete data set.}
 	\label{fig:discrete_mle_hist}
\end{figure}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{discrete_mle_prob}
 	\caption{Likelihood function $L$ of data in discrete data set. Probability is zero at undefined states.}
 	\label{fig:discrete_mle_prob}
\end{figure}

\section{Gaussian distribution}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{normal_mle}
 	\caption{Histogram of randomly generated data from normal distribution $\mathcal{N}(40,32^2)$. The green curve is a plot of normal distribution with the maximum likelihood estimate of $\theta$ parameters.}
 	\label{fig:normal_mle}
\end{figure}

Normal distribution is defined by $\theta = \{\mu, \sigma^2\}$. The goal is to find parameter $\theta \in \Theta$, such that the product in \ref{eq:gaussian_mle_prod} is maximized.

\begin{equation} \label{eq:gaussian_mle_prod}
\prod_{j=1}^{M} L(x_j, \theta)
\end{equation}

In case of Gaussian distribution, the likelihood function $L$ is defined by

\begin{equation}
L(x, \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2 \sigma^2}},
\end{equation}

where $x \in X$. Taking derivative of \ref{eq:gaussian_mle_prod} with respect to $\mu$ equal to 0 yields

\begin{equation}
\sum_{j=1}^{M}{x_j - M \mu} = 0.
\end{equation}

Maximum likelihood estimate of $\mu$ is therefore

\begin{equation}
\mu_{\text{est}} = \overline X_M.
\end{equation}

Derivative of \ref{eq:gaussian_mle_prod} with respect to $\sigma^2$ equal to zero results in MLE of $\sigma^2$ to be

\begin{equation}
\sigma^2_{\text{est}} = \frac{1}{M} \sum_{j=1}^{M} {(x_j-\overline X_M)^2}.
\end{equation}

Figure \ref{fig:normal_mle} shows an example of MLE on normally distributed random variable.

\section{Gaussian mixture distribution}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{gaussian_mixture_estimate}
 	\caption{Histogram of randomly generated data from two normal distributions $\mathcal{N}(10,5^2)$ and $\mathcal{N}(30,4^2)$. The green curve is the best estimate, found using EM algorithm, of underlying normal mixture distribution. Dotted curves are estimates of individual Gaussian mixture components.}
 	\label{fig:gaussian_mixture_est}
\end{figure}

Given $K$ components, there are $K$ component parameters $\theta_1, \theta_2, \dots, \theta_K$ and $K$ weights $w_1, w_2, \dots, w_K$ to estimate. Here I provide description of Expectation-Maximization algorithm based on lecture notes by Padhraic Smyth \cite{gaussian-mixture-em}. This algorithm provides an estimate of the aforementioned parameters.

Let's define a membership weight $w_{jk}$, of data point $x_j$ in component $k$ as

\begin{equation}
w_{jk} = \frac{\alpha_k \text{PDF}_{\mathcal{N}(\theta_k)}(x_j)}{\sum_{i=1}^K \alpha_i \text{PDF}_{\mathcal{N}(\theta_i)}(x_j)},
\end{equation}

where $\alpha_k$ is a probability that a randomly selected $x_j$ was generated by component $k$. $w_{jk}$ can be thought of as a level of certainty that $x_j$ was generated by component $k$. $\sum_{k=1}^K w_{jk} = 1$ holds.

Likelihood of all data points given gaussian mixture parameters is defined as

\begin{equation} \label{eq:gaussian_mixture_likelihood}
l = \sum_{j=1}^{M} \sum_{k=1}^{K} \alpha_k \text{PDF}_{\mathcal{N}(\theta_k)}(x_j).
\end{equation}

EM is an iterative algorithm, typically iterating until convergence is detected. Each iteration consists of two parts. An E-step and an M-step.

In the E-step, weight $w_{jk}$ is computed for each data point $x_j$ and component $k$.

Define $W_k = \sum_{j=1}^{M} w_{jk}$, the weight of all data points in component $k$. In the M-step, new model parameters are calculated as follows:

\begin{equation}
\alpha_k^{\text{new}} = \frac{W_k}{M},
\end{equation}

\begin{equation}
\mu_k^{\text{new}} = \frac{\sum_{j=1}^M w_{jk} x_j}{W_k},
\end{equation}

\begin{equation}
{\sigma_k^{2}}^{\text{new}} = \frac{\sum_{j=1}^M w_{jk} (x_j-\mu_k^{\text{new}})^2}{W_k}.
\end{equation}

Where $\mu_k^{\text{new}}$ resembles standard equation for estimating mean, and 
${\sigma_k^{2}}^{\text{new}}$ is similar to an equation for estimating variance. The differences are in weighting.

Before the iterations begin, an initial guess of model parameters is necessary. This can be chosen randomly or through a heuristic. One issue with the EM algorithm is that it does not guarantee to find a global optimum.

Termination of the algorithm is determined by checking that the likelihood \ref{eq:gaussian_mixture_likelihood} of all data points hasn't improved enough in between iterations. In another words

\begin{equation}
l_{\text{cur}} - l_{\text{prev}} < \text{tol}.
\end{equation}

\chapter{Analysis}

\section{Markov chain}

\subsection{Introduction}
Markov chain describes possible sequences of events in which a probability of being at a state at given time depends only on value of previous state. 

\subsection{Description}

A Markov chain is defined by

\begin{itemize}
\item set of states $S = \{S_1, S_2, \dots, S_N\}$,
\item transition matrix $A \in \textbf R^{N \times N}$,
\item initial probability vector $\pi \in \textbf R^N$.
\end{itemize}

Where $a_{ij}, 1 \leq i,j \leq N$ is a probability of transitioning from state $S_i$ into state $S_j$. $\pi_i$ is a probability that the initial state is $S_i$.

Suppose a sequence of states $y_1, y_2, \dots, y_T$, where $T \geq 2$. At time $t$, where $1 \leq t \leq T-1$, the transition probability $a_{ij}$ is defined as $P(y_{t+1}=S_j | y_t=S_i)$.

\subsection{Markov property}
Markov chain satisfies a Markov property. This property says that $P(y_{t+1}=S_i|y_1=s_1,y_2=s_2,\dots,y_t=s_t) = P(y_{t+1}=S_{i}|y_t=s_t)$. In another words, being at state $S_i$ at time $t+1$ only conditionally depends on being at state $s_t$ at time $t$. Further history doesn't influence the probability.

\subsection{Example}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{mc}
 	\caption{Visualization of a Markov chain with three states.}
 	\label{fig:mc}
\end{figure}

As an example, consider a Markov chain (MC) with three states: $S_1 = \text{sunny}$, $S_2 = \text{rainy}$ and $S_3 = \text{cloudy}$. This MC models how weather changes on every day. Suppose the following transition matrix:

\begin{equation}
A =
\begin{bmatrix}
	0.6	& 0.1 & 0.3 \\
	0.1 & 0.7 & 0.2 \\
	0.3 & 0.3 & 0.4
\end{bmatrix}.
\end{equation}

If the weather is sunny, tomorrow will be most likely sunny as well. With probability 0.3 it will be cloudy. Rainy weather is very unlikely tomorrow. Consider the following initial probability vector:

\begin{equation}
\pi = \begin{bmatrix} 0.3 \\ 0.2 \\ 0.5 \end{bmatrix}.
\end{equation}

The most likely weather on first day of measurement is cloudy.

\subsection{Limitations}

The previous example only takes into account weather at time $t$ to predict weather at time $t+1$. In reality, there are multiple factors that influence what the weather will be like. According to the example, if a weather is sunny now, it is unlikely it will start raining. If we, however, take into account humidity, we can improve the prediction. If we know that humidity is high, even though it was sunny, we can predict it started raining. Hidden Markov Model addresses this issue.

\section{Hidden Markov Model}

\subsection{Introduction}

Hidden Markov Model (HMM) is an extension of Markov chain. The states are now called hidden states and every hidden state has an associated observed state.

\subsection{Description}

HMM is defined by: (based on paper by L. Rabiner \cite{hmm-lawrence})
\begin{itemize}

\item set of hidden states $S = \{S_1, S_2, \dots, S_N\}$,
\item set of observed symbols $X$,
\item transition matrix $A \in \textbf R^{N \times N}$,
\item observation probability functions $B = \{b_1(o), b_2(o), \dots, b_N(o)\}$,
\item initial probability vector $\pi \in \textbf R^N$.

\end{itemize}

The values of the $X$ can be either discrete or continuous. The values of $S$ are assumed to be discrete. The observation probability function $b_i(o)$ describes a probability $P(x_t = o | y_t = S_i)$. This probability function is also called an emission function.

Imagine a walk through a HMM graph (see figure \ref{fig:hmm_graph} for an example graph). At time point $t$, there is a hidden state $y_t$ and an observed symbol $x_t$. Value of the observed symbol conditionally depends on value of the hidden state. The dependency is described by emission function $b_i(o)$.

If $t \neq 1$ and $T > 1$, there is a hidden state $y_t$ that conditionally depends on value of previous hidden state $y_{t-1}$. The dependency is described by transition $a_{ij}$. If $t = 1$, there is a hidden state $y_1$ that conditionally depends on initial transition $\pi_i$.

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{hmm}
 	\caption{Visualization of a walk through a Hidden Markov Model. At each time point, there is a hidden state $y_t$ and an observed symbol $x_t$.}
 	\label{fig:hmm}
\end{figure}

\subsection{Operations}

There are three common operations that can be performed on HMM. Learning, inference and scoring.

\subsubsection{Learning}

Learning is used to estimate the model parameters $\{\pi, A, B\} = \lambda$. It is desired to estimate the parameters such that $\prod_{j=1}^{M} P(\textbf x_j | \lambda )$ is maximized. Where $M \in \textbf N$ is a number of sequences to be learned. $\textbf x_j$ is a \emph{j}-th sequence of observed symbols. In another words, it is desired to maximize the probability that given sequences of observed symbols were generated by the model.

\subsubsection{Inference}

Inference returns the most likely sequence of hidden states, given a sequence of observed symbols.

\subsubsection{Scoring}

Given a sequence of observed symbols $x_1, x_2, \dots, x_T$, what is the probability that the sequence was generated by HMM with parameters $\lambda$? Scoring answers this question.

\subsection{Learning}

\subsubsection{MLE/EM approach}

\paragraph{Initial transition function}

The initial probability vector $\pi$ can be viewed as a probability function $\pi_i$, assigning a probability to every hidden state $S_i \in S$. This is a function of discrete random variable that defines discrete probability distribution. As such, MLE technique (discussed in chapter \ref{parameter-estimation-chapter}) can be used to estimate the distribution parameters.

\paragraph{Transition function}

Every row of $A$ defines a probability function $a_{ij}$ with parameter $j$. These functions are called transition functions. The function describes a probability of transitioning into state $S_j$ form state $S_i$. The transition functions define discrete probability distributions. Parameters of every such distribution can be estimated using MLE.

\paragraph{Emission function}

The observation probability functions $b_i(o)$ are functions of either discrete or continuous random variable that takes on values in set of observed symbols $X$. For purposes of this thesis, it is assumed that continuous variables have Gaussian or Gaussian mixture distribution.

The functions $b_i(o)$ are also called emission functions (emissions). If values of the set of observed symbols $X$ are discrete, then the emissions define discrete distributions and the parameters can be estimated through MLE. If the values of $X$ are continuous, then the emissions define continuous probability distributions. As discussed earlier, parameters of Gaussian distribution can be estimated using MLE. Parameters of Gaussian mixture distribution can be estimated through EM algorithm.

\subsection{Inference}

Given a sequence of observed symbols, what is the most likely associated sequence of hidden states? Viterbi algorithm answers this question. Let's define $\alpha_t(i)$ to be the maximum probability of sequence of hidden states and observed symbols up to time point $t$:

\begin{equation} \label{eq:hmm_viterbi_a}
\alpha_t(i) = max_{y_1,y_2,\dots,y_{t-1}} P(y_1,y_2,\dots,y_{t-1},y_t = S_i,x_1,x_2,\dots,x_t | \lambda),
\end{equation}

where $\lambda$ are parameters of HMM model.

\paragraph{Initialization}

At time point $t=1$, the probability of being at hidden state $S_i$ is simply the initial probability of being at that state and a probability of being at observed state $x_1$ given hidden state $S_i$. In another words:

\begin{equation}
\alpha_1(i) = \pi_i b_i(x_1).
\end{equation}

\paragraph{Recursion}
Taking into account the structure of HMM, the equation \ref{eq:hmm_viterbi_a} can be rewritten recursively as

\begin{equation} \label{eq:hmm_viterbi_a_rec}
\alpha_t(j) = [\max_i \alpha_{t-1}(i)a_{ij}] b_j(x_t).
\end{equation}

Hidden state at particular time point $t$ is determined by

\begin{equation}
s_t = S_{\argmax_i \alpha_{t}(i)}.
\end{equation}

\paragraph{Complexity}

At each time point $t$ and hidden state index $j$, the equation \ref{eq:hmm_viterbi_a_rec} loops through every hidden state index $i$. There are $N$ hidden states. In addition, the recursion continues for every time point $t$. There are $T$ time points. Thus the overall complexity is
$N^2 T$.

\subsection{Scoring}

In previous section, I have defined $\alpha_t(i)$ to be a probability of a sequence of hidden states up to time point $t$, where $y_t = S_i$, and a sequence of observed symbols up to time point $t$, given model parameters.

In order to compute the probability that sequence of observed symbols was generated by the HMM (score), take a look how $\alpha_T(i)$ is defined:

\begin{equation} \label{eq:hmm_scoring_alpha}
\alpha_T(i) = max_{y_1,y_2,\dots,y_{T-1}} P(y_1,y_2,\dots,y_{T-1},y_T = S_i,x_1,x_2,\dots,x_T | \lambda).
\end{equation}

The score is then

\begin{equation} \label{eq:hmm_score}
\sum_{i=1}^{N} \alpha_T(i).
\end{equation}

\subsection{Example}

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{hmm-graph}
 	\caption{Visualization of a Hidden Markov Model with three states and a continuous observed variable.}
 	\label{fig:hmm_graph}
\end{figure}

Suppose an extension of an example mentioned in Markov chain chapter. i.e. there are three hidden states of weather: $S_1 = \text{sunny}$, $S_2 = \text{rainy}$ and $S_3 = \text{cloudy}$. Lets introduce a continuous observed random variable describing humidity level. It is easy to imagine that on sunny day, the humidity will be typically lower than on a rainy day. This observable variable can help in predicting future weather.

\subsection{Limitations}
There is only one observed variable, which may not be sufficient for successful prediction in real-life applications. One could increase the dimensionality of the obsereved variable. That would, however, lead to a problem called curse of dimensionality. As we would increase the dimensionality of observed variable, the learning time would increase greatly up to the point where it would no longer be tractable. Dynamic naive Bayesian classifier addresses this issue.

\section{Bayesian network}

Bayesian network is a probabilistic graphical model consisting of nodes and edges. The model is a directed acyclic graph (DAG). Every node describes a random variable which can be either discrete or continuous. Edges describe a conditional dependency among the nodes (variables).

As an example, consider a Bayesian network in figure \ref{fig:bn}. Every node represents a discrete random variable with two possible states - $true$ and $false$. It can be imagined that dehydration can cause red skin, but it can also influence presence of cancer. In addition, red skin may signal a lack of water intake, or the skin may be red because of cancer cells. Every node defines a conditional probability table (CPT) given nodes it depends on.

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{bn}
 	\caption{Visualization of a Bayesian network with three states.}
 	\label{fig:bn}
\end{figure}

\begin{table}
\parbox{0.45\linewidth}{
\centering
\begin{tabular}{|c|c|}
\hline
\multicolumn{2}{|c|}{dehydration}  \\
\hline
true & false \\
\hline
0.1 & 0.9 \\
\hline
\end{tabular}
\caption{CPT for $dehydration$.}
}
\hfill
\parbox{0.45\linewidth}{
\centering
\begin{tabular}{|c||c|c|}
\hline
input & \multicolumn{2}{c|}{cancer} \\
\hline
dehydration & true & false \\
\hline
true & 0.1 & 0.9 \\
\hline
false & 0.01 & 0.99 \\
\hline
\end{tabular}
\caption{CPT for $cancer$.}
}
\centering
\begin{tabular}{|c|c||c|c|}
\hline
\multicolumn{2}{|c||}{input} & \multicolumn{2}{c|}{red skin} \\
\hline
cancer & dehydration & true & false \\
\hline
true & true & 0.4 & 0.6 \\
\hline
true & false & 0.3 & 0.7 \\
\hline
false & true & 0.4 & 0.6 \\
\hline
false & false & 0.1 & 0.9 \\
\hline
\end{tabular}
\caption{CPT for $red$ $skin$.}
\end{table}

\section{Naive Bayesian classifier}

Naive Bayesian classifier is a classifier (model that assigns class labels to given inputs) where features (random variables) are assumed to be conditionally independent from each other.

Imagine we would like to classify a class $C_k$ given feature vector $\textbf x$:

\begin{equation}
P(C_k | x_1, x_2, \dots, x_n).
\end{equation}

Using Bayes' theorem, the conditional probability can be rewritten as

\begin{equation}
P(C_k | \textbf x) = \frac{P(C_k,\textbf x)}{P(\textbf x)} = \frac{P(C_k) P(\textbf x | C_k)}{P(\textbf x)}.
\end{equation}

Because the features are assumed to be conditionally independent from each other, the probability in numerator can be rewritten as

\begin{equation}
P(\textbf x | C_k) = P(x_1|C_k) P(x_2|C_k) \cdots P(x_n|C_k).
\end{equation}

Because of the independence assumption, learning of the classifier can be performed effectively, as mentioned in artice by K. M. Leung \cite{naive-bayesian-classifier}.

\section{Dynamic Bayesian network}

Dynamic Bayesian network is a Bayesian network which accounts for time. At each time point $t$, the values of random variables can be computed based on values of associated random variables in time $t-1$.

Consider figure \ref{fig:dbn} as an example. It can be imagined that velocity of an airplane at time $t$ conditionally depends on velocity at time $t-1$ as well as on wind speed at time $t-1$. Remaining gas at time $t$ conditionally depends on remaining gas at $t-1$ and it is also influenced by altitude and velocity at current time $t$.

\begin{figure}
	\centering
 	\includegraphics[width=1\textwidth]{dbn}
 	\caption{Visualization of a Dynamic Bayesian network with four variables at time $t$.}
 	\label{fig:dbn}
\end{figure}

\section{Dynamic naive Bayesian classifier}

\subsection{Introduction}

Dynamic naive Bayesian classifier (DNBC) is an extension of Hidden Markov Model (HMM). The difference is that there may be a number of observed variables. In contrast, HMM is defined for only one observed variable. DNBC is dynamic, because it classifies sequences with variables at every time $t$. It is also naive, because the output variables are assumed to be conditionally independent from each other. Figure \ref{fig:dnbc} demonstrates a walk through a model with 2 observed variables. 

\subsection{Description}

DNBC is defined by

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{dnbc}
 	\caption{Visualization of a Dynamic naive Bayesian classifier with two observed variables. At each time point, there is a hidden state $y_t$ and two observed states: $x_t^1$ and $x_t^2$.}
 	\label{fig:dnbc}
\end{figure}

\begin{itemize}

\item{number of observed variables $M$,}
\item set of hidden states $S = \{S_1, S_2, \dots, S_N\}$,
\item{sets of observed symbols $X^j, j \in [1,M]$}
\item transition matrix $A \in \textbf R^{N \times N}$,
\item observation probability functions $B^j = \{b_1^j(o), b_2^j(o), \dots, b_N^j(o)\}, j \in [1,M]$,
\item initial probability vector $\pi \in \textbf R^N$.

\end{itemize}

The values of particular set of observed symbols $X^j$ can be either discrete or continuous. The values of $S$ are assumed to be discrete. The observation probability function $b_i^j(o)$ describes a probability $P(x^j_t=o|y_t=S_i)$. $b_i^j(o)$ is called an emission function.

Imagine a walk through DNBC graph. At time point $t$, there is a hidden state $y_t$ and $M$ observed symbols $x^j_t$ where $j \in [1,M]$. Value of every observed symbol conditionally depends on value of the hidden state. The dependency is described by emission function $b_i^j(o)$.

An important property of DNBC is that the observed variables are assumed to be independent from each other. Therefore, DNBC does not define any conditional dependency function between two observed variables.

If $t \neq 1$ and $T > 1$, there is a hidden state $y_t$ that conditionally depends on value of previous hidden state $y_{t-1}$. The dependency is described by transition $a_{ij}$. If $t = 1$, there is a hidden state $y_1$ that conditionally depends on initial transition $\pi_i$.

\subsection{Operations}

There are three common operations that can be performed on DNBC. Learning, inference and scoring.

\subsubsection{Learning}

Learning is used to estimate the model parameters $\{\pi,A,B^1,B^2,\dots,B^M\} = \lambda$. It is desired to estimate the parameters such that $\prod_{j=1}^{M} P(\textbf x_j | \lambda )$ is maximized. Where $M \in \textbf N$ is a number of sequences to be learned. $\textbf x_j$ is a \emph{j}-th sequence of observed symbols. In another words, it is desired to maximize the product of probabilities that given sequence of observed symbols was generated by given model.

\subsubsection{Inference}

Inference returns the most likely sequence of hidden states, given sequences of observed symbols. Each sequence of observed symbols corresponds to particular observed variable. Each sequence thus has the same number of elements.

\subsubsection{Scoring}

Given sequences of observed symbols $x_1^j, x_2^j, \dots, x_T^j, j \in [1,M]$ (one sequence per observed variable), what is the probability that the sequences were generated by DNBC with parameters $\lambda$? Scoring answers this question.

\subsection{Learning}

\subsubsection{Maximum likelihood estimation}

This learning approach is similar to one described in case of HMM. The only difference is that there are multiple emission functions $b^j_i(o)$ where $j \in [1,M]$ and $M$ is the number of observed variables. Therefore, there is now $M \times N$ continuous distributions to be learned. Each distribution is assumed to be Gaussian or Gaussian mixture.

\subsection{Inference}

Inference algorithm is again Viterbi. It is simply extended to support multiple observed variables. Let $\textbf{x}_t^j$ be a sequence of observed symbols of variable $j$ up to time point $t$.

\begin{equation}
\textbf{x}_t^j = x_1^j,x_2^j,\dots,x_t^j.
\end{equation}

Let's define $a_t(i)$ to be the maximum probability of sequence of hidden and sets of observed symbols up to time point $t$:

\begin{equation} \label{eq:dnbc_viterbi_a}
a_t(i) = max_{y_1,y_2,\dots,y_{t-1}} P(y_1,y_2,\dots,y_{t-1},y_t = S_i,\textbf{x}_t^1,\textbf{x}_t^2,\dots,\textbf{x}_t^M| \lambda),
\end{equation}

where $\lambda$ are parameters of DNBC model.

\paragraph{Initialization}

At time point $t=1$, the probability of being at hidden state $S_i$ is equal to the initial probability of being at that state and probabilities of being at observed symbol $x_1^j$ given hidden state $S_i$. In another words:

\begin{equation}
\alpha_1(i) = \pi_i \prod_{j=1}^M b^j_i(x^j_1).
\end{equation}

\paragraph{Recursion}
Taking into account the structure of DNBC, the equation \ref{eq:dnbc_viterbi_a} can be rewritten recursively as

\begin{equation} \label{eq:dnbc_viterbi_a_rec}
\alpha_t(j) = [\max_i \alpha_{t-1}(i)a_{ij}] \prod_{k=1}^M b_j^k(x^k_t).
\end{equation}

Hidden state at particular time point $t$ is determined by

\begin{equation}
s_t = S_{\argmax_i \alpha_{t}(i)}.
\end{equation}

\paragraph{Complexity}

At each time point $t$ and hidden state index $j$, the equation \ref{eq:dnbc_viterbi_a_rec} loops through every hidden state index $i$ and every observed variable index $k$. There are $N$ hidden states and $M$ observed variables. In addition, the recursion continues for every time point $t$. There are $T$ time points. Thus the overall time complexity is
$N(N+M) T$.

\subsection{Scoring}

Scoring is performed the same way as in case of HMM. That is, the score can be calculated as

\begin{equation} \label{eq:hmm_score}
\sum_{i=1}^{N} \alpha_T(i).
\end{equation}

\subsection{Example}

Consider a problem of weather prediction. Let's define a set of hidden states to be $S = \{S_1 = \text{sunny}, S_2 = \text{rainy}, S_3 = \text{cloudy} \}$. Let's consider the following observed variables: temperature, humidity and windiness. At every time $t$, the hidden state conditionally depends on hidden state at time $t-1$. The three hidden variables at time $t$ conditionally depend on hidden state $t$. These variables can help in predicting the hidden state.

\subsection{Limitations}

The observed variables are assumed to be conditionally independent from each other. While this assumption enables the use of algorithms with relatively low time complexity, it also limits the ability to describe more complicated relations among random variables. For example, the observed variables may be correlated in reality.

\chapter{Implementation}

I have implemented a Dynamic naive Bayesian Classifier in Scala programming language that runs on top of Apache Spark. I named this software \textit{dnbc-scala}.

\section{Scala language}

Scala is a programming language that combines functional and object-oriented programming. Scala code compiles into Java Virtual Machine (JVM) bytecode. This enables one to incorporate existing Java libraries into a Scala project.

\section{Apache Spark}

Apache Spark is an engine for cluster-computing. It provides Application Programming Interface (API) that developers can use to execute computations on multiple nodes - processors or machines in a cluster. Spark is also fault-tolerant, meaning that if a computation fails at given node, another node will take over.

\section{Project structure}

The project is divided into 5 modules:

\begin{itemize}

\item \textit{core} - main module containing DNBC interface, helper class for loading data set into required format and a class for measuring performance of DNBC on a data set,
\item \textit{core-test} - contains tests of $core$ module,
\item \textit{performance} - provides command line interface for generating a parametric data set and measuring DNBC performance on it,
\item \textit{preprocessing} - transforms a predefined data set into data sets with different parameters,
\item \textit{test-utils} - provides commonly used functions in tests or in performance measurement.

\end{itemize}

Figure \ref{fig:module-dependencies} provides overview of the project structure and module dependencies.

\begin{figure}
	\centering
 	\includegraphics[width=0.7\textwidth]{module-dependencies}
 	\caption{Dependencies among modules in \textit{dnbc-scala} project.}
 	\label{fig:module-dependencies}
\end{figure}

\section{Usage}

\paragraph{DynamicNaiveBayesianClassifier object}

\textit{DynamicNaiveBayesianClassifier} object in \textit{core} module serves as a factory for already learned class \textit{DynamicNaiveBayesianClassifier}.

\begin{lstlisting}[language=Scala]
object DynamicNaiveBayesianClassifier {
  def mle(sc: SparkContext,
          sequences: Iterable[Seq[State]],
          continuousVariableHints: Option[List[Int]] = Option.empty)
          : DynamicNaiveBayesianClassifier = {
          // ...
          }
}
\end{lstlisting}

The function \textit{mle} returns a learned (using MLE) model. \textit{sequences} are sequences of \textit{States} used for learning. Optional argument \textit{continuousVariableHints} can be provided to set the number of components in gaussian mixtures. The implementation does not try to estimate the number. If the parameter is not provided, the continuous variables are expected to have normal distribution.

The model parameters (\textit{Edges}) are learning in every sequence of states. After all the sequences have been processed, learning is finalized by calling \textit{learnFinalize} on every edge. These functions are executed in parallel since they are not data-dependent with each other.

\paragraph{DynamicNaiveBayesianClassifier class}

This class represents a DNBC model with known parameters.

\begin{lstlisting}[language=Scala]
class DynamicNaiveBayesianClassifier(
initialEdge: LearnedDiscreteEdge,
transitions: Map[String, LearnedDiscreteEdge],
discreteEmissions: List[Map[String, LearnedDiscreteEdge]],
continuousEmissions: List[Map[String, LearnedContinuousEdge]]) {
def inferMostLikelyHiddenStates(observedStates: Seq[ObservedState]): List[String]
def score(observedStates: Seq[ObservedState]): Double
}
\end{lstlisting}

The \textit{inferMostLikelyHiddenStates} function returns the most likely sequence of hidden states given a sequence of observed states. I am using Viterbi algorithm. The following is a function for Viterbi initialization.

\begin{lstlisting}[language=Scala]
  private def viterbiInitialize(observedStates: Seq[ObservedState]): Map[String,Double] = {
    var vcur = Map.empty[String,Double]
    for (hiddenState <- transitions.keys) {
      var emissionsSum = 0.0
      emissionsSum += discreteEmissions.zipWithIndex.map(z => Math.log(z._1(hiddenState)
        .probability(observedStates.head.DiscreteVariables(z._2)))).sum
      emissionsSum += continuousEmissions.zipWithIndex.map(z => Math.log(z._1(hiddenState)
        .probability(observedStates.head.ContinuousVariables(z._2)))).sum
      vcur += (hiddenState -> (emissionsSum + Math.log(initialEdge.probability(hiddenState))))
    }
    vcur
  }
\end{lstlisting}

The \textit{score} function returns a log-probability that given sequence of observed states was generated by the DNBC. It is internally using Viterbi algorithm to compute $\alpha_T(i)$.

The Viterbi algorithm is not implemented in parallel. The reason is that the algorithm proceeds by time points, and each time point depends on the previous one. At every time point, the parallelization is not worth it, since there is only a bunch of additions taking place.

\paragraph{State}

An observed state at given time point consists of observations of all the discrete and continuous variables. This is represented by \textit{ObservedState} class.

\begin{lstlisting}[language=Scala]
class ObservedState(discreteVariables: List[String], continuousVariables: List[Double]) {
  def DiscreteVariables: List[String] = discreteVariables
  def ContinuousVariables: List[Double] = continuousVariables
}
\end{lstlisting}

A state at given time point consists of a hidden state and an \textit{ObservedState}. Throughout the project, the hidden states are of \textit{String} type.

\begin{lstlisting}[language=Scala]
class State(hiddenState: String, observedState: ObservedState) {
  def HiddenState: String = hiddenState
  def ObservedState: ObservedState = observedState
}
\end{lstlisting}

\paragraph{Edge}

An edge represents a particular random variable, parameters of which are to be learned.

\begin{lstlisting}[language=Scala]
trait Edge[T] {
  def learn(occurrence: T): Unit
  def learnFinalize(): LearnedEdge[T]
}
\end{lstlisting}

The \textit{learn} function notifies an edge that \textit{occurence} occured. User calls \textit{learnFinalize} function to estimate the variable parameters based on previous occurences.

There are two kinds of edges: \textit{DiscreteEdge} and \textit{ContinuousEdge}. As expected, \textit{DiscreteEdge} represents a discrete random variable, whereas \textit{ContinuousEdge} represents a continuous random variable.

To estimate the parameters of continuous random variables using MLE, I am using jMEF Java library. Previously I used Spark's MLlib library for estimating parameters of a Gaussian Mixture Model (GMM). The problem with this is approach was that it can only estimate the parameters by making a computation across multiple nodes in a cluster. This is not a desired behavior since there are many estimations of small data that should run in parallel instead. Therefore, I ended up using jMEF library which estimates the parameters in sequential fashion.

\paragraph{LearnedEdge}

A \textit{LearnedEdge} represents a random variable which parameters have been already estimated. The \textit{probability} function returns a probability that a state occurs.

\begin{lstlisting}[language=Scala]
trait LearnedEdge[T] {
  def probability(state: T): Double
}
\end{lstlisting}

\paragraph{RandomEdge}

A \textit{RandomEdge} generates random value given random variable parameters. This is useful for performance measurement, where data sets are randomly generated.

\begin{lstlisting}[language=Scala]
trait RandomEdge[T] {
  def next(): T
}
\end{lstlisting}

\section{Underflow}

Since the Viterbi algorithm is computing products of probabilities, the resulting value gets small in few iterations. To avoid underflow, one can compute a log-probability instead, using the following trick:

\begin{equation}
\log(abc) = \log(a)+\log(b)+\log(c).
\end{equation}

Now instead of multiplying tiny numbers together, one can use addition.

\chapter{Experiments}

\chapter{Návrh}

\chapter{Realizace}

\begin{conclusion}
	%sem napište závěr Vaší práce
\end{conclusion}

\bibliographystyle{csn690}
\bibliography{bibliography}

\appendix

\chapter{Seznam použitých zkratek}
% \printglossaries
\begin{description}
	\item[GUI] Graphical user interface
	\item[XML] Extensible markup language
\end{description}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % Tuto kapitolu z výsledné práce ODSTRAŇTE.
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% 
% \chapter{Návod k~použití této šablony}
% 
% Tento dokument slouží jako základ pro napsání závěrečné práce na Fakultě informačních technologií ČVUT v~Praze.
% 
% \section{Výběr základu}
% 
% Vyberte si šablonu podle druhu práce (bakalářská, diplomová), jazyka (čeština, angličtina) a kódování (ASCII, \mbox{UTF-8}, \mbox{ISO-8859-2} neboli latin2 a nebo \mbox{Windows-1250}). 
% 
% V~české variantě naleznete šablony v~souborech pojmenovaných ve formátu práce\_kódování.tex. Typ může být:
% \begin{description}
% 	\item[BP] bakalářská práce,
% 	\item[DP] diplomová (magisterská) práce.
% \end{description}
% Kódování, ve kterém chcete psát, může být:
% \begin{description}
% 	\item[UTF-8] kódování Unicode,
% 	\item[ISO-8859-2] latin2,
% 	\item[Windows-1250] znaková sada 1250 Windows.
% \end{description}
% V~případě nejistoty ohledně kódování doporučujeme následující postup:
% \begin{enumerate}
% 	\item Otevřete šablony pro kódování UTF-8 v~editoru prostého textu, který chcete pro psaní práce použít -- pokud můžete texty s~diakritikou normálně přečíst, použijte tuto šablonu.
% 	\item V~opačném případě postupujte dále podle toho, jaký operační systém používáte:
% 	\begin{itemize}
% 		\item v~případě Windows použijte šablonu pro kódování \mbox{Windows-1250},
% 		\item jinak zkuste použít šablonu pro kódování \mbox{ISO-8859-2}.
% 	\end{itemize}
% \end{enumerate}
% 
% 
% V~anglické variantě jsou šablony pojmenované podle typu práce, možnosti jsou:
% \begin{description}
% 	\item[bachelors] bakalářská práce,
% 	\item[masters] diplomová (magisterská) práce.
% \end{description}
% 
% \section{Použití šablony}
% 
% Šablona je určena pro zpracování systémem \LaTeXe{}. Text je možné psát v~textovém editoru jako prostý text, lze však také využít specializovaný editor pro \LaTeX{}, např. Kile.
% 
% Pro získání tisknutelného výstupu z~takto vytvořeného souboru použijte příkaz \verb|pdflatex|, kterému předáte cestu k~souboru jako parametr. Vhodný editor pro \LaTeX{} toto udělá za Vás. \verb|pdfcslatex| ani \verb|cslatex| \emph{nebudou} s~těmito šablonami fungovat.
% 
% Více informací o~použití systému \LaTeX{} najdete např. v~\cite{wikilatex}.
% 
% \subsection{Typografie}
% 
% Při psaní dodržujte typografické konvence zvoleného jazyka. České \uv{uvozovky} zapisujte použitím příkazu \verb|\uv|, kterému v~parametru předáte text, jenž má být v~uvozovkách. Anglické otevírací uvozovky se v~\LaTeX{}u zadávají jako dva zpětné apostrofy, uzavírací uvozovky jako dva apostrofy. Často chybně uváděný symbol "{} (palce) nemá s~uvozovkami nic společného.
% 
% Dále je třeba zabránit zalomení řádky mezi některými slovy, v~češtině např. za jednopísmennými předložkami a spojkami (vyjma \uv{a}). To docílíte vložením pružné nezalomitelné mezery -- znakem \texttt{\textasciitilde}. V~tomto případě to není třeba dělat ručně, lze použít program \verb|vlna|.
% 
% Více o~typografii viz \cite{kobltypo}.
% 
% \subsection{Obrázky}
% 
% Pro umožnění vkládání obrázků je vhodné použít balíček \verb|graphicx|, samotné vložení se provede příkazem \verb|\includegraphics|. Takto je možné vkládat obrázky ve formátu PDF, PNG a JPEG jestliže používáte pdf\LaTeX{} nebo ve formátu EPS jestliže používáte \LaTeX{}. Doporučujeme preferovat vektorové obrázky před rastrovými (vyjma fotografií).
% 
% \subsubsection{Získání vhodného formátu}
% 
% Pro získání vektorových formátů PDF nebo EPS z~jiných lze použít některý z~vektorových grafických editorů. Pro převod rastrového obrázku na vektorový lze použít rasterizaci, kterou mnohé editory zvládají (např. Inkscape). Pro konverze lze použít též nástroje pro dávkové zpracování běžně dodávané s~\LaTeX{}em, např. \verb|epstopdf|.
% 
% \subsubsection{Plovoucí prostředí}
% 
% Příkazem \verb|\includegraphics| lze obrázky vkládat přímo, doporučujeme však použít plovoucí prostředí, konkrétně \verb|figure|. Například obrázek \ref{fig:float} byl vložen tímto způsobem. Vůbec přitom nevadí, když je obrázek umístěn jinde, než bylo původně zamýšleno -- je tomu tak hlavně kvůli dodržení typografických konvencí. Namísto vynucování konkrétní pozice obrázku doporučujeme používat odkazování z~textu (dvojice příkazů \verb|\label| a \verb|\ref|).
% 
% \begin{figure}\centering
% 	\includegraphics[width=0.5\textwidth, angle=30]{cvut-logo-bw}
% 	\caption[Příklad obrázku]{Ukázkový obrázek v~plovoucím prostředí}\label{fig:float}
% \end{figure}
% 
% \subsubsection{Verze obrázků}
% 
% % Gnuplot BW i barevně
% Může se hodit mít více verzí stejného obrázku, např. pro barevný či černobílý tisk a nebo pro prezentaci. S~pomocí některých nástrojů na generování grafiky je to snadné.
% 
% Máte-li například graf vytvořený v programu Gnuplot, můžete jeho černobílou variantu (viz obr. \ref{fig:gnuplot-bw}) vytvořit parametrem \verb|monochrome dashed| příkazu \verb|set term|. Barevnou variantu (viz obr. \ref{fig:gnuplot-col}) vhodnou na prezentace lze vytvořit parametrem \verb|colour solid|.
% 
% \begin{figure}\centering
% 	\includegraphics{gnuplot-bw}
% 	\caption{Černobílá varianta obrázku generovaného programem Gnuplot}\label{fig:gnuplot-bw}
% \end{figure}
% 
% \begin{figure}\centering
% 	\includegraphics{gnuplot-col}
% 	\caption{Barevná varianta obrázku generovaného programem Gnuplot}\label{fig:gnuplot-col}
% \end{figure}
% 
% 
% \subsection{Tabulky}
% 
% Tabulky lze zadávat různě, např. v~prostředí \verb|tabular|, avšak pro jejich vkládání platí to samé, co pro obrázky -- použijte plovoucí prostředí, v~tomto případě \verb|table|. Například tabulka \ref{tab:matematika} byla vložena tímto způsobem.
% 
% \begin{table}\centering
% 	\caption[Příklad tabulky]{Zadávání matematiky}\label{tab:matematika}
% 	\begin{tabular}{|l|l|c|c|}\hline
% 		Typ		& Prostředí		& \LaTeX{}ovská zkratka	& \TeX{}ovská zkratka	\tabularnewline \hline \hline
% 		Text		& \verb|math|		& \verb|\(...\)|	& \verb|$...$|		\tabularnewline \hline
% 		Displayed	& \verb|displaymath|	& \verb|\[...\]|	& \verb|$$...$$|	\tabularnewline \hline
% 	\end{tabular}
% \end{table}
% 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\chapter{Obsah přiloženého CD}

%upravte podle skutecnosti

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{stručný popis obsahu CD}.
		.1 exe\DTcomment{adresář se spustitelnou formou implementace}.
		.1 src.
		.2 impl\DTcomment{zdrojové kódy implementace}.
		.2 thesis\DTcomment{zdrojová forma práce ve formátu \LaTeX{}}.
		.1 text\DTcomment{text práce}.
		.2 thesis.pdf\DTcomment{text práce ve formátu PDF}.
		.2 thesis.ps\DTcomment{text práce ve formátu PS}.
	}
\end{figure}

\end{document}
